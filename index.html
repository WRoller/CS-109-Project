<!DOCTYPE html>

<html>
    <head>
    	<!-- CSS Content -->
    	<link rel="stylesheet" type="text/css" href="css/bootstrap.css">
		<link rel="stylesheet" type="text/css" href="css/style.css">

        <!-- JS Content -->
		<script src="http://code.jquery.com/jquery-1.9.1.js"></script>
		<script src="http://code.jquery.com/ui/1.10.2/jquery-ui.js"></script>
        <script src="js/bootstrap.js"></script>
        <script src="js/dropdown.js"></script>
		<script src="js/script.js"></script>
    </head>

    <body>
        <div class = "navbar navbar-inverse navbar-fixed-top" role="navigation">
            <div class="header_bar">
                <span><img id="alien" src="images/alien2.png"></span>
                            Reddit Rankings
                <span><img id="alien" src="images/alien2.png"></span>
            </div>
        </div>


        <div class="jumbotron" id="overview">
            <h2>Overview and Motivation</h2>
            <div class = "col-md-4">
                <iframe width="80%" height="300" src="http://www.youtube.com/embed/lGXQ8mQMR0s" frameborder="0" allowfullscreen></iframe>
            </div>
            <div class = "col-md-8 text_box">
                <div>
                    <b>Project Goal:</b><br><u>To create a recommender for Reddit.</u> When you visit Reddit, you can visit "subreddits" such as "funny" or "worldnews" and view recent/top/trending submissions there. But what if you want to do a "StumbleUpon"-type search and just keep clicking through to more Reddit posts without worrying about what subcategory you are in? Well.....<br><br>
                </div>
                
                <div>
                    <b>Motivation:</b><br>Reddit has been calling for developers to help them create a new recommender --
                    <i>"One of reddit's greatest strengths is the huge collection of niche communities and categories of content that we have. One of our greatest weaknesses is that most of it never makes it to the front page. So many vast, undiscovered communities. We have loads and loads of these communities, some very tiny, but they just aren't very discoverable. I think that helping people find this stuff is a problem worth solving, and so do plenty of researchers and grad students that have contacted us asking for this data"</i><br>
                    <a href="http://www.reddit.com/r/announcements/comments/ddz0s/reddit_wants_your_permission_to_use_your_data_for/">-Reddit's Call for Data</a>
                </div>

            </div>
        </div>

        <div class="jumbotron" id = "related_work">
            <h2>Data Overview</h2>
            <div class = "col-md-8 text_box1">        
                    1. <b>Scrape top subreddits categories:</b><br>
                    -We used http://www.redditlist.com/, which lists the top 125 Reddit subcategories by "Recent Activity", "Subscribers", and "Growth (past 24 hours)". We chose the top 125 subreddits by Subscribers so that the submission came from a larger timeframe and represented the most popular over time.<br>
                    <b>2. Build datasets for analysis: </b><br>
                    -This meant pulling subreddits -> submissions -> comments.We decided to pull the top 125 subReddits, the 100 top submissions for each SubReddit, and the  10 'best'comments for each submission. We split the information into two datasets: user-specific and submission-specific info.<br>
                    <b>The two data sets consisted of the following variables:</b><br>
                    <u>userlist:</u> username, link_karma, comment_karma, total # comments made (according to our set), redditor since<br>
                    <u>commentlist: </u>comment author, comment text, comment score, comment ups, comment downs, comment post date, item id, item score, item author, item post date, item downvotes, item upvotes, item total # comments<br><br>

                    <b><i>Why did we go for "best" versus "top" comments?</i></b><br>
                    Reddit has a new sorting algorithm called "best" to solve the problem of "top" comments being only those from the first few hours after a submission was posted:<br>
                    <div align="center"><i>"reddit is heavily biased toward comments posted early. When a mediocre joke gets posted in the first hour a story is up, it will become the top comment if it's even slightly funny. The reason for this bias is that once a comment gets a few early upvotes, it's moved to the top. The higher something is listed, the more likely it is to be read (and voted on), and the more votes the comment gets. It's a feedback loop that cements the comment's position, and a comment posted an hour later has little chance of overtaking it -- even if people reading it are upvoting it at a much higher rate." </i>-<a href="http://blog.reddit.com/2009/10/reddits-new-comment-sorting-system.html"><br>xkcd guest-blogging on Reddit</a><br><BR></div>
                    <i>Spoiler: this algorithm for sorting comments inspired us later on for a way to recommend new content</i>
            </div>
            <div class = "col-md-4">
                <img class = "graph2" src="Viz/comments_per_user.png">
                <h3><u>Summary Statistics</u></h3>
                <b>Unique Subreddits: 116</b><br>
                <b>Unique Users: 85,336</b><br> 
                <b>Unique Comments: 110,775</b><br>
                <br>
            </div>
        </div>
        <div class="jumbotron" id = "explore">
            <h2>Exploratory Data Visualizations</h2>
                <img class = "graph" src="Viz/net_vote_hist.png">
                <img class = "graph" src="Viz/up_vote_hist.png">
                <img class = "graph" src="Viz/time_score_scatter.png">
                <img class = "graph" src="Viz/up_net_scatter.png">
            </div>
                
            
        </div>

        <div class="jumbotron" id = "model_1">
            <h2>Model 1: The Network Approach</h2>
            <div class = "col-md-4">
                <img class = "equation" src="images/model_1.PNG">
            </div>
            <div class = "col-md-8 text_box">
               
                Let's take a specific user, loook at all the articles they've commented, and then look at other users who have commented on those same articles, going out 2 degrees of separation of mutual commenters to include in our calculation. We will then rate each of those comments and give them a score, weighted by # of Up Votes / Total Score. We will feed this "score", which is a probability, into a random binomial generator to calculate the confidence interval of that probability, and then take the confidence interval's lower-bound. <br>
                <br>
                Still looking at this specific user, we will rank all the comments in the whole network (of 2 degrees of separation) by this lower-bound score, and recommend the top 10 unique articles given from the highest comment ratings once sorted.<br>
            </div>
                
        </div>  

        <div class="jumbotron" id = "model_2">
            <h2>Model 2: The Item-Based Approach</h2>
            <div class = "col-md-8 text_box2">
                This model, on the other hand, does not calculate on the fly, and instead, it builds a database. It first groups all the comments dataset by the submission link on which a user has commented. Then, it creates a list of all the users who have commented on a single submission. This list is then turned  into a single string, on which we run "Bag of Words", to generate a binary indicator vector for each submission link. After that, we will have a matrix of N unique users by M unique submissions. Each entry in one row of the matrix corresponds to one user of that row's specific submission. The entry will be 0 if the specific user has not commented on that row's particular article, and 1 if he/she has. <br>
                We then look at every possible combo of 2 links in the matrix to calculate the similarity score. This is done by 
                calculating the "cosine distance" between 2 links-- which is essentially, the scaled dot product, normalized to be between 0 and 1:<br>

                This concept of "cosine distance" comes from Amazon, which uses "<a href = "http://en.wikipedia.org/wiki/Slope_One">binary collaborative filtering</a>" as one of its recommendation systems, for when only binary data is available, not user's ratings for a particular article or product. In Amazon's case the binary data is whether a shopper did or did not purchase a product after viewing, and for us it was whether a user commented or not.
                <br>
                Our recommentation function takes a username, searches for all the articles that particular user has commented on, and fetches all the similarity scores for each article that the user has commented on and each other articles in database. Sorting by highest similarity score, this model returns the top n recommendations.<br><br>
            </div>
            <div class = "col-md-4">
                <img class = "equation" src="images/model_2.PNG">
                <div>
                    Where <b>A</b> and <b>B</b> are indicator Vectors for which users commented (viewed) the corresponding Reddit links. 
                </div>
            </div>

        </div>  

        <div class="jumbotron" id ="user_example">
            <h2>User Example</h2>
            <div class="row-fluid">
                <div class="span3 user">
                    <div class="btn-group">
                        <div id="user_id">User</div>   
                      <button type="button" class="btn btn-default dropdown-toggle" data-toggle="dropdown" id = "selected_user" >
                        <div id = "selected_user">Select User</div>
                      </button>
                      <ul class="dropdown-menu" role="menu">
                        <li> <a class = "user_examples 1" id="user_1" >way_fairer</a></li>
                        <li><a class = "user_examples 2" id="user_2">Warlizard</a></li>
                        <li><a class = "user_examples 3" id="user_3">I_are_facepalm</a></li>
                        <li><a class = "user_examples 4" id="user_4">kickme444</a></li>
                        <li><a class = "user_examples 5" id="user_5">nowhathappenedwas</a></li>
                      </ul>
                    </div>

                </div>
                <div class="span3 user">
                    Content Interest
                    <table class="table">
                        <tr><td ><a class = "current_links 1" href="javascript:void(0)" target="_blank">Link 1</a></td></tr>
                        <tr><td><a class = "current_links 2" href="javascript:void(0)" target="_blank">Link 2</td></tr>
                        <tr><td><a class = "current_links 3" href="javascript:void(0)" target="_blank">Link 3</td></tr>
                        <tr><td><a class = "current_links 4" href="javascript:void(0)" target="_blank">Link 4</td></tr>
                        <tr><td><a class = "current_links 5" href="javascript:void(0)" target="_blank">Link 5</td></tr>
                    </table>
                </div>
                <div class="span3 user">
                    Database Approach
                    <table class="table">
                        <tr><td ><a class = "rec_1 1" href="javascript:void(0)" target="_blank">Link 1</a></td></tr>
                        <tr><td><a class = "rec_1 2" href="javascript:void(0)" target="_blank">Link 2</td></tr>
                        <tr><td><a class = "rec_1 3" href="javascript:void(0)" target="_blank">Link 3</td></tr>
                        <tr><td><a class = "rec_1 4" href="javascript:void(0)" target="_blank">Link 4</td></tr>
                        <tr><td><a class = "rec_1 5" href="javascript:void(0)" target="_blank">Link 5</td></tr>
                    </table>

                </div>
                <div class="span3 user">
                    Network Approach
                    <table class="table">
                        <tr><td ><a class = "rec_2 1" href="javascript:void(0)" target="_blank">Link 1</a></td></tr>
                        <tr><td><a class = "rec_2 2" href="javascript:void(0)" target="_blank">Link 2</td></tr>
                        <tr><td><a class = "rec_2 3" href="javascript:void(0)" target="_blank">Link 3</td></tr>
                        <tr><td><a class = "rec_2 4" href="javascript:void(0)" target="_blank">Link 4</td></tr>
                        <tr><td><a class = "rec_2 5" href="javascript:void(0)" target="_blank">Link 5</td></tr>
                    </table>

                </div>
            </div>
        </div>
        
        <div class="jumbotron" id ="future_research">
            <h2>Ideas and Features for the Future</h2>
            <div class="text_box2">
                <b>1. Get Feedback:</b>
                Create actual "Stumble Upon" functionality by embedding the submission in the page (rather than providing a URL) and allowing the viewer to like/dislike the given submission. This would then update the prior similarity scores teach the recommender about the user's preferences.  In the current models, we make the assumption that articles on which a user has commented are the type of articles that the user would like to see more of.  However, this does not always seem to be the case, as comment chains on Reddit often devolve into discussions entirely unrelated to the original submission (as we've discovered over the course of this project).  While we currenly use comments as an informative prior, in the future we'd hope to improve our recommendations by incorporating user feedback based on actual "likes" and "dislikes"<BR>
       
                <div>
                <b>2. Evolving Database:</b>
                Our current recommender is based on a static set of Reddit comments that were pulled based on top Reddit articles in each sub-Reddit at a specific point in time.  Obviously this set of comments becomes less relevent with each passing day, but are still the foundation of our recommendation systems.  This static database also makes it impossible for us to recommend articles that were posted after the date on which we scraped our data, which is a huge shortcoming.  In the future we'd love to make the database dynamic, and update it each day to incorporate new articles and comments.  Also, the data only represent a small subset of the entire comment history on Reddit (all of what we were able to pull due to computing limitations), so may be a biased sample. <BR>
                <b>3. Scalability:</b>
                Mentioned above, one of the major problems we ran into was processing and scraping the massive volume of data available on Reddit.  We were only able to get Model 2 to run on a subset of the full comment data (the way the code above is written, recommendations are based only on comments pulled from sub-Reddits 0-50 out of 125).  The code below shows our attempt to build the database using MRJob.  The first piece of code shows the data manipulation that was necessary to format the data in such a way that MRJob would be able to process it by writing it to a csv file, which took nearly as long as building the small database itself.  The second piece of code is (almost complete) MRJob code that would build the database based on the entire set of comment data.  Due to time constraints we opted to perfect our recommenders on smaller subsets of data rather than rush to scale them to be able to run on the whole data set.  In the future we would love to be able to generate a database for all Reddit comments!
                </div>
            </div>
        </div>


        <div class="jumbotron" id = "about_us">
            <h2>About Us</h2>
            <div class="row-fluid">
                <div class="span3 authors">
                    <img class = "author_image" src = "images/caroline.jpg">
                    <div>Caroline</div>
                    <div class="about_details">Caroline is senior economics concentrator working towards a secondary in  computer science. She will graduate in Spring 2014. Caroline enjoys  candle-making and gathering Pokemon cards in her free time. </div>
                </div>
                <div class="span3 authors">
                    <img class = "author_image" src = "images/val.jpg">
                    <div>Valerie</div>
                    <div class="about_details">Valerie is a senior statistics concentrators. Val likes to make dreamcatchers out of Harvard dinning hall serving implements. She will graduate in Spring 2014. </div>
                </div>
                <div class="span3 authors">
                    <img class = "author_image" src = "images/wes.jpg">
                    <div>Wes</div>
                    <div class="about_details">Wes is a senior statistics concentrator. In addition to being an avid connoisseur of jokes, Wes likes parkour. He will graduate in Spring 2014.  </div>
                </div>
                <div class="span3 authors">
                    <img class = "author_image" src = "images/will.jpg">
                    <div>Will</div>
                    <div class="about_details">Will is senior economics concentrator working towards a secondary in  computer science. Will likes to balance rocks along the roadside. He will graduate in Spring 2014. </div>
                </div>
            </div>       
        </div>

        <div class="jumbotron footer" id = "foot">

            <div id="downloads">

                <span class = "data_download" >
                    <a href = "data.zip " target="_blank">
                        <img  class = "down_item" src = "images/data.jpg">
                        <div class='data_name'>Main .csv Dataset</div>
                    </a>
                </span>

                <span class = "data_download">    
                    <a href = "Process_Book.zip" target="_blank">
                        <img class = "down_item" src = "images/ipython.png">
                        <div class='data_name'>iPython Notebook</div>
                    </a>
                </span>

            </div>

            <div class="footer" id='copyright'>
                Project By: Valerie Bradley, Caroline Davis, Weston Sterns, & Will Roller
            </div>  
        </div>
    </body>   
</html>